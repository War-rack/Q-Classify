{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67555dd9-e48d-4359-9488-392294c306bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.42.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (3.2.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.17.1)\n",
      "Requirement already satisfied: autograd in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.8.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray<0.8,>=0.6.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.7.2)\n",
      "Requirement already satisfied: cachetools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (5.3.3)\n",
      "Requirement already satisfied: pennylane-lightning>=0.42 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.42.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (2.32.5)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.13.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (23.2)\n",
      "Requirement already satisfied: diastatic-malt in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (2.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy-openblas32>=0.3.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane-lightning>=0.42->pennylane) (0.3.30.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: astunparse in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Requirement already satisfied: gast in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (0.6.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2025.1.31)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn numpy pandas joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984c1e6e-ca3a-4c81-bfc7-9a81d775391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: (10151, 2)\n",
      "TF-IDF shapes: (6686, 2000) (1434, 2000) (2031, 2000)\n",
      "Classical SVM val acc: 0.9686192468619247\n",
      "Classical SVM test acc: 0.9650418513047759\n",
      "PCA reduced shapes: (6686, 6) (1434, 6) (2031, 6)\n",
      "Random search: 40 trials, subset size 300\n",
      "  Trial 1/40 alignment = 0.200130\n",
      "  Trial 2/40 alignment = 0.200130\n",
      "  Trial 3/40 alignment = 0.200130\n",
      "  Trial 4/40 alignment = 0.200130\n",
      "  Trial 5/40 alignment = 0.200130\n",
      "  Trial 6/40 alignment = 0.200130\n",
      "  Trial 7/40 alignment = 0.200130\n",
      "  Trial 8/40 alignment = 0.200130\n",
      "  Trial 9/40 alignment = 0.200130\n",
      "  Trial 10/40 alignment = 0.200130\n",
      "  Trial 11/40 alignment = 0.200130\n",
      "  Trial 12/40 alignment = 0.200130\n",
      "  Trial 13/40 alignment = 0.200130\n",
      "  Trial 14/40 alignment = 0.200130\n",
      "  Trial 15/40 alignment = 0.200130\n",
      "  Trial 16/40 alignment = 0.200130\n",
      "  Trial 17/40 alignment = 0.200130\n",
      "  Trial 18/40 alignment = 0.200130\n",
      "  Trial 19/40 alignment = 0.200130\n",
      "  Trial 20/40 alignment = 0.200130\n",
      "  Trial 21/40 alignment = 0.200130\n",
      "  Trial 22/40 alignment = 0.200130\n",
      "  Trial 23/40 alignment = 0.200130\n",
      "  Trial 24/40 alignment = 0.200130\n",
      "  Trial 25/40 alignment = 0.200130\n",
      "  Trial 26/40 alignment = 0.200130\n",
      "  Trial 27/40 alignment = 0.200130\n",
      "  Trial 28/40 alignment = 0.200130\n",
      "  Trial 29/40 alignment = 0.200130\n",
      "  Trial 30/40 alignment = 0.200130\n",
      "  Trial 31/40 alignment = 0.200130\n",
      "  Trial 32/40 alignment = 0.200130\n",
      "  Trial 33/40 alignment = 0.200130\n",
      "  Trial 34/40 alignment = 0.200130\n",
      "  Trial 35/40 alignment = 0.200130\n",
      "  Trial 36/40 alignment = 0.200130\n",
      "  Trial 37/40 alignment = 0.200130\n",
      "  Trial 38/40 alignment = 0.200130\n",
      "  Trial 39/40 alignment = 0.200130\n",
      "  Trial 40/40 alignment = 0.200130\n",
      "Best subset alignment: 0.2001295645154093\n",
      "Attempting local Nelder-Mead refinement (scipy required)...\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.200130\n",
      "         Iterations: 44\n",
      "         Function evaluations: 225\n",
      "Refinement done. new subset alignment: 0.20012956451540934\n",
      "Learned params shape: (3, 6)\n",
      "Computing statevectors for full training set...\n",
      "  C=0.1 CV acc = 0.8929\n",
      "  C=1 CV acc = 0.9098\n",
      "  C=10 CV acc = 0.9227\n",
      "  C=50 CV acc = 0.9252\n",
      "Best C: 50\n",
      "QSVM Test accuracy: 0.9202363367799113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1610\n",
      "           1       0.82      0.79      0.80       421\n",
      "\n",
      "    accuracy                           0.92      2031\n",
      "   macro avg       0.88      0.87      0.88      2031\n",
      "weighted avg       0.92      0.92      0.92      2031\n",
      "\n",
      "Computing ensemble (average probabilities)...\n",
      "Classical SVM test acc: 0.9650\n",
      "QSVM test acc: 0.9202, Ensemble test acc: 0.9591\n",
      "Elapsed time: 2134.0 s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "qsvm_spam_ham_fixed.py\n",
    "\n",
    "QSVM pipeline (quantum-kernel SVM) for spam/ham classification with a robust,\n",
    "non-differentiable randomized search for feature-map parameter tuning (fixes\n",
    "autograd vdot VJP issues).\n",
    "\n",
    "Save and run:\n",
    "    pip install pennylane scikit-learn numpy pandas joblib\n",
    "    python qsvm_spam_ham_fixed.py\n",
    "\n",
    "This script:\n",
    " - Loads your uploaded datasets (/mnt/data/spam_ham_dataset.csv and /mnt/data/spam.csv if present)\n",
    " - TF-IDF -> PCA -> scale to [-pi, pi] for AngleEmbedding\n",
    " - Defines a parameterized quantum feature map that returns statevectors\n",
    " - Uses randomized search (and optional local Nelder-Mead refine) to maximize kernel-target alignment\n",
    " - Computes fidelity kernel |<psi(x)|psi(z)>|^2 and trains an SVM on the precomputed kernel\n",
    " - Saves artifacts under ./artifacts/\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Pennylane\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as qnp\n",
    "except Exception as e:\n",
    "    raise ImportError(\"This script requires pennylane. Install with: pip install pennylane\") from e\n",
    "\n",
    "# ----------------------------\n",
    "# USER CONFIG\n",
    "# ----------------------------\n",
    "DATA_PATHS = [\"spam_ham_dataset.csv\", \"spam.csv\"]\n",
    "ARTIFACTS_DIR = \"artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "try:\n",
    "    qnp.random.seed(RND)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "n_qubits = 6                # number of qubits (PCA components)\n",
    "feature_map_layers = 3      # parameterized layers in the feature map\n",
    "random_search_trials = 40   # number of random samples to try during search\n",
    "random_search_subsample = 300  # # train samples used to evaluate alignment per trial\n",
    "local_refine = True         # if True, attempts Nelder-Mead refine (requires scipy)\n",
    "local_refine_iters = 300\n",
    "\n",
    "tfidf_max_features = 2000\n",
    "pca_components = n_qubits\n",
    "\n",
    "device_name = \"default.qubit\"  # use statevector simulator for kernel fidelity\n",
    "svm_C_grid = [0.1, 1, 10, 50]\n",
    "\n",
    "# ----------------------------\n",
    "# Data load & preprocessing\n",
    "# ----------------------------\n",
    "def load_and_merge(paths):\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"Info: {p} not found, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(p, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "        if \"text\" in df.columns and (\"label\" in df.columns or \"label_num\" in df.columns):\n",
    "            if \"label_num\" not in df.columns:\n",
    "                df[\"label_num\"] = df[\"label\"].map(lambda s: 1 if str(s).strip().lower().startswith(\"spam\") else 0)\n",
    "            frames.append(df[[\"text\", \"label_num\"]].rename(columns={\"label_num\":\"label\"}))\n",
    "        elif \"v2\" in df.columns and \"v1\" in df.columns:\n",
    "            frames.append(pd.DataFrame({\n",
    "                \"text\": df[\"v2\"].astype(str),\n",
    "                \"label\": df[\"v1\"].apply(lambda s: 1 if str(s).strip().lower()==\"spam\" else 0)\n",
    "            }))\n",
    "        else:\n",
    "            cols = list(df.columns)\n",
    "            if len(cols) >= 2:\n",
    "                temp = df[[cols[0], cols[1]]].rename(columns={cols[0]:\"text\", cols[1]:\"label\"})\n",
    "                try:\n",
    "                    temp[\"label\"] = temp[\"label\"].astype(int)\n",
    "                except Exception:\n",
    "                    temp[\"label\"] = temp[\"label\"].apply(lambda s: 1 if str(s).strip().lower().startswith(\"spam\") else 0)\n",
    "                frames.append(temp)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(f\"No suitable dataset found among {paths}\")\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    df_all[\"text\"] = df_all[\"text\"].astype(str).str.strip()\n",
    "    df_all = df_all.drop_duplicates(subset=[\"text\"])\n",
    "    df_all = df_all.dropna(subset=[\"text\",\"label\"])\n",
    "    df_all[\"label\"] = df_all[\"label\"].astype(int)\n",
    "    print(\"Merged dataset:\", df_all.shape)\n",
    "    return df_all\n",
    "\n",
    "def vectorize_and_split(df):\n",
    "    texts = df[\"text\"].values\n",
    "    labels = df[\"label\"].values\n",
    "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(texts, labels, test_size=0.20, stratify=labels, random_state=RND)\n",
    "    X_train_txt, X_val_txt, y_train, y_val = train_test_split(X_train_txt, y_train, test_size=0.1765, stratify=y_train, random_state=RND)\n",
    "    tfv = TfidfVectorizer(max_features=tfidf_max_features, stop_words=\"english\")\n",
    "    X_train_tfidf = tfv.fit_transform(X_train_txt).toarray()\n",
    "    X_val_tfidf = tfv.transform(X_val_txt).toarray()\n",
    "    X_test_tfidf = tfv.transform(X_test_txt).toarray()\n",
    "    joblib.dump(tfv, os.path.join(ARTIFACTS_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "    print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape)\n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, X_train_txt, X_val_txt, X_test_txt\n",
    "\n",
    "# ----------------------------\n",
    "# Quantum feature map qnode (returns statevector)\n",
    "# ----------------------------\n",
    "dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def state_circuit(params, x):\n",
    "    \"\"\"\n",
    "    params: array shape (feature_map_layers, n_qubits)\n",
    "    x: array shape (n_qubits,) scaled into [-pi, pi]\n",
    "    returns: statevector (complex numpy array) via qml.state()\n",
    "    \"\"\"\n",
    "    # embed classical features\n",
    "    qml.templates.AngleEmbedding(x, wires=range(n_qubits), rotation=\"X\")\n",
    "    for l in range(params.shape[0]):\n",
    "        # trainable single qubit rotations (RY)\n",
    "        for w in range(n_qubits):\n",
    "            qml.RY(params[l, w], wires=w)\n",
    "        # entangling ring\n",
    "        for w in range(n_qubits):\n",
    "            qml.CNOT(wires=[w, (w + 1) % n_qubits])\n",
    "    return qml.state()\n",
    "\n",
    "# ----------------------------\n",
    "# Kernel utilities (numpy-based; no autograd)\n",
    "# ----------------------------\n",
    "def compute_states(params, X):\n",
    "    \"\"\"Compute statevectors for all rows in X (X is numpy array shape (N, n_qubits)).\"\"\"\n",
    "    states = []\n",
    "    for x in X:\n",
    "        st = state_circuit(params, x)  # returns pennylane numpy array, convert to np.array\n",
    "        states.append(np.array(st))\n",
    "    return np.array(states)  # shape (N, dim) complex\n",
    "\n",
    "def fidelity_kernel_from_states(states_A, states_B=None):\n",
    "    \"\"\"Compute fidelity kernel K_ij = |<a_i|b_j>|^2 using numpy (real float).\"\"\"\n",
    "    if states_B is None:\n",
    "        states_B = states_A\n",
    "    N = states_A.shape[0]\n",
    "    M = states_B.shape[0]\n",
    "    K = np.zeros((N, M), dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            ov = np.vdot(states_A[i], states_B[j])\n",
    "            K[i, j] = np.abs(ov) ** 2\n",
    "    return K\n",
    "\n",
    "# ----------------------------\n",
    "# Kernel-target alignment (numpy) and randomized search optimizer\n",
    "# ----------------------------\n",
    "def kernel_target_alignment_score_from_params(params, X_subset, y_subset):\n",
    "    \"\"\"\n",
    "    Compute kernel-target alignment for given params on subset.\n",
    "    y_subset: {0,1} -> converted to +/-1 internally.\n",
    "    Returns alignment (float).\n",
    "    \"\"\"\n",
    "    states = compute_states(params, X_subset)\n",
    "    K = fidelity_kernel_from_states(states)\n",
    "    y_pm1 = 2 * np.array(y_subset) - 1\n",
    "    yyT = np.outer(y_pm1, y_pm1)\n",
    "    numerator = np.sum(K * yyT)\n",
    "    denom = np.linalg.norm(K) * (np.linalg.norm(y_pm1) ** 2 + 1e-12)\n",
    "    alignment = numerator / (denom + 1e-12)\n",
    "    return float(alignment)\n",
    "\n",
    "def optimize_feature_map_random_search(X_train_scaled, y_train,\n",
    "                                       init_params=None,\n",
    "                                       n_trials=40,\n",
    "                                       subsample=300,\n",
    "                                       random_state=RND,\n",
    "                                       local_refine=False):\n",
    "    \"\"\"\n",
    "    Randomized search for best feature-map params by maximizing kernel-target alignment\n",
    "    on a random subset (uses numpy-based evaluation; no autograd).\n",
    "    If local_refine True, attempts Nelder-Mead refine around the best candidate (requires scipy).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_train = X_train_scaled.shape[0]\n",
    "    subs_idx = rng.choice(n_train, min(subsample, n_train), replace=False)\n",
    "    X_sub = X_train_scaled[subs_idx]\n",
    "    y_sub = np.array(y_train)[subs_idx]\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    # scale for random sampling around init\n",
    "    base_sigma = 0.5 if init_params is None else 0.2\n",
    "\n",
    "    print(f\"Random search: {n_trials} trials, subset size {len(subs_idx)}\")\n",
    "    for t in range(n_trials):\n",
    "        if init_params is None:\n",
    "            cand = 0.5 * rng.randn(feature_map_layers, n_qubits)\n",
    "        else:\n",
    "            cand = np.array(init_params) + base_sigma * rng.randn(*init_params.shape)\n",
    "        try:\n",
    "            score = kernel_target_alignment_score_from_params(cand, X_sub, y_sub)\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {t+1} evaluation failed: {e}\")\n",
    "            score = -np.inf\n",
    "        print(f\"  Trial {t+1}/{n_trials} alignment = {score:.6f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = cand.copy()\n",
    "\n",
    "    print(\"Best subset alignment:\", best_score)\n",
    "\n",
    "    if local_refine and (best_params is not None):\n",
    "        try:\n",
    "            print(\"Attempting local Nelder-Mead refinement (scipy required)...\")\n",
    "            from scipy.optimize import minimize\n",
    "\n",
    "            def obj_flat(xflat):\n",
    "                x = xflat.reshape(feature_map_layers, n_qubits)\n",
    "                return -kernel_target_alignment_score_from_params(x, X_sub, y_sub)  # minimize negative alignment\n",
    "\n",
    "            x0 = best_params.flatten()\n",
    "            res = minimize(obj_flat, x0, method=\"Nelder-Mead\", options={\"maxiter\": local_refine_iters, \"disp\": True})\n",
    "            best_params = res.x.reshape(feature_map_layers, n_qubits)\n",
    "            print(\"Refinement done. new subset alignment:\", kernel_target_alignment_score_from_params(best_params, X_sub, y_sub))\n",
    "        except Exception as e:\n",
    "            print(\"Local refinement failed (scipy missing or error):\", e)\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# ----------------------------\n",
    "# Train QSVM with learned kernel\n",
    "# ----------------------------\n",
    "def train_qsvm_with_learned_kernel(params, X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    print(\"Computing statevectors for full training set...\")\n",
    "    states_train = compute_states(params, X_train_scaled)\n",
    "    K_train = fidelity_kernel_from_states(states_train)\n",
    "\n",
    "    # Grid-search C using precomputed kernel cross-validation\n",
    "    best_score = -1\n",
    "    best_C = None\n",
    "    best_clf = None\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RND)\n",
    "    for C in svm_C_grid:\n",
    "        clf = SVC(kernel=\"precomputed\", C=C)\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in skf.split(K_train, y_train):\n",
    "            K_tr = K_train[train_idx][:, train_idx]\n",
    "            K_val = K_train[val_idx][:, train_idx]\n",
    "            clf.fit(K_tr, y_train[train_idx])\n",
    "            y_val_pred = clf.predict(K_val)\n",
    "            cv_scores.append(accuracy_score(y_train[val_idx], y_val_pred))\n",
    "        avg = np.mean(cv_scores)\n",
    "        print(f\"  C={C} CV acc = {avg:.4f}\")\n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            best_C = C\n",
    "            best_clf = clf\n",
    "\n",
    "    print(\"Best C:\", best_C)\n",
    "\n",
    "    # Fit final classifier on full training kernel\n",
    "    final_clf = SVC(kernel=\"precomputed\", C=best_C, probability=True)\n",
    "    final_clf.fit(K_train, y_train)\n",
    "\n",
    "    # compute test kernel slice\n",
    "    states_test = compute_states(params, X_test_scaled)\n",
    "    K_test_train = fidelity_kernel_from_states(states_test, states_train)\n",
    "\n",
    "    y_test_pred = final_clf.predict(K_test_train)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"QSVM Test accuracy:\", test_acc)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Save artifacts\n",
    "    joblib.dump({\"params\": params, \"states_train\": states_train, \"K_train\": K_train}, os.path.join(ARTIFACTS_DIR, \"qkernel_artifacts.joblib\"))\n",
    "    joblib.dump(final_clf, os.path.join(ARTIFACTS_DIR, \"qsvm_clf.joblib\"))\n",
    "    return test_acc, final_clf, K_train, states_train\n",
    "\n",
    "# ----------------------------\n",
    "# Optional classical baseline & ensemble\n",
    "# ----------------------------\n",
    "def train_classical_svm(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    svc = SVC(kernel=\"linear\", C=1.0, probability=True, random_state=RND)\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(\"Classical SVM val acc:\", accuracy_score(y_val, svc.predict(X_val)))\n",
    "    print(\"Classical SVM test acc:\", accuracy_score(y_test, svc.predict(X_test)))\n",
    "    joblib.dump(svc, os.path.join(ARTIFACTS_DIR, \"classical_svm.joblib\"))\n",
    "    return svc\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN pipeline\n",
    "# ----------------------------\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    df = load_and_merge(DATA_PATHS)\n",
    "    X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, \\\n",
    "        X_train_txt, X_val_txt, X_test_txt = vectorize_and_split(df)\n",
    "\n",
    "    # classical baseline\n",
    "    svc = train_classical_svm(X_train_tfidf, y_train, X_val_tfidf, y_val, X_test_tfidf, y_test)\n",
    "\n",
    "    # PCA reduce & scale to [-pi, pi]\n",
    "    pca = PCA(n_components=pca_components, random_state=RND)\n",
    "    X_train_pca = pca.fit_transform(X_train_tfidf)\n",
    "    X_val_pca = pca.transform(X_val_tfidf)\n",
    "    X_test_pca = pca.transform(X_test_tfidf)\n",
    "    joblib.dump(pca, os.path.join(ARTIFACTS_DIR, \"pca.joblib\"))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-np.pi, np.pi))\n",
    "    X_train_pca_scaled = scaler.fit_transform(X_train_pca)\n",
    "    X_val_pca_scaled = scaler.transform(X_val_pca)\n",
    "    X_test_pca_scaled = scaler.transform(X_test_pca)\n",
    "    joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, \"pca_scaler.joblib\"))\n",
    "\n",
    "    print(\"PCA reduced shapes:\", X_train_pca_scaled.shape, X_val_pca_scaled.shape, X_test_pca_scaled.shape)\n",
    "\n",
    "    # initialize random params (small)\n",
    "    init_params = 0.1 * np.random.randn(feature_map_layers, n_qubits)\n",
    "\n",
    "    # Randomized search (robust, avoids autograd issues)\n",
    "    learned_params = optimize_feature_map_random_search(\n",
    "        X_train_pca_scaled, y_train,\n",
    "        init_params=init_params,\n",
    "        n_trials=random_search_trials,\n",
    "        subsample=random_search_subsample,\n",
    "        random_state=RND,\n",
    "        local_refine=local_refine\n",
    "    )\n",
    "    print(\"Learned params shape:\", learned_params.shape)\n",
    "    joblib.dump(learned_params, os.path.join(ARTIFACTS_DIR, \"learned_featuremap_params.joblib\"))\n",
    "\n",
    "    # Train QSVM\n",
    "    qsvm_test_acc, final_clf, K_train, states_train = train_qsvm_with_learned_kernel(\n",
    "        learned_params, X_train_pca_scaled, y_train, X_test_pca_scaled, y_test\n",
    "    )\n",
    "\n",
    "    # Optional ensemble with classical SVM\n",
    "    print(\"Computing ensemble (average probabilities)...\")\n",
    "    svm_probs = svc.predict_proba(X_test_tfidf)[:, 1]\n",
    "    # get QSVM probabilities by fitting probability=True SVC on precomputed kernel (we already did)\n",
    "    # final_clf was trained with probability=True, so we can use it directly\n",
    "    states_test = compute_states(learned_params, X_test_pca_scaled)\n",
    "    K_test_train = fidelity_kernel_from_states(states_test, states_train)\n",
    "    qsvm_probs = final_clf.predict_proba(K_test_train)[:, 1]\n",
    "\n",
    "    ensemble_probs = 0.5 * (svm_probs + qsvm_probs)\n",
    "    ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "    ensemble_acc = accuracy_score(y_test, ensemble_preds)\n",
    "    print(f\"Classical SVM test acc: {accuracy_score(y_test, svc.predict(X_test_tfidf)):.4f}\")\n",
    "    print(f\"QSVM test acc: {qsvm_test_acc:.4f}, Ensemble test acc: {ensemble_acc:.4f}\")\n",
    "    print(\"Elapsed time: %.1f s\" % (time.time() - t0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f84a0-a0ab-4792-af3c-ce05edbcd6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
