{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3634f3-5968-46a2-aa67-c104d372ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.42.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (3.2.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.17.1)\n",
      "Requirement already satisfied: autograd in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.8.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray<0.8,>=0.6.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.7.2)\n",
      "Requirement already satisfied: cachetools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (5.3.3)\n",
      "Requirement already satisfied: pennylane-lightning>=0.42 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.42.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (2.32.5)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (0.13.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (23.2)\n",
      "Requirement already satisfied: diastatic-malt in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane) (2.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy-openblas32>=0.3.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pennylane-lightning>=0.42->pennylane) (0.3.30.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: astunparse in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Requirement already satisfied: gast in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (0.6.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\dell\\anaconda3\\lib\\site-packages (from diastatic-malt->pennylane) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pennylane) (2025.1.31)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane scikit-learn numpy pandas joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d85d27-6bd9-4c90-9866-f1ec7a5e9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Pennylane\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    from pennylane import numpy as qnp\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Install pennylane: pip install pennylane\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f464dff9-128b-47cc-8bec-148011ad86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# USER TUNABLE FLAGS (speed vs accuracy)\n",
    "# -----------------------------\n",
    "FAST_MODE = True  # set False if you want more accuracy and don't mind time\n",
    "VERBOSE = True\n",
    "\n",
    "# Base random seed\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "try:\n",
    "    qnp.random.seed(RND)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Artifacts dir\n",
    "ARTIFACTS = \"artifacts\"\n",
    "os.makedirs(ARTIFACTS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61e5494-d2ad-4688-b191-3cf0dfcd24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters (fast defaults)\n",
    "# -----------------------------\n",
    "if FAST_MODE:\n",
    "    TFIDF_MAX_FEATURES = 500   # smaller TF-IDF -> fewer PCA comps needed\n",
    "    N_QUBITS = 4               # fewer qubits -> huge speedup\n",
    "    N_LAYERS = 1               # smaller ansatz\n",
    "    EPOCHS = 20                # small but with larger stepsize and bigger batches\n",
    "    BATCH_SIZE = 128\n",
    "    STEPSIZE = 0.12            # larger step\n",
    "    EARLY_STOPPING = 4         # stop earlier\n",
    "else:\n",
    "    TFIDF_MAX_FEATURES = 2000\n",
    "    N_QUBITS = 6\n",
    "    N_LAYERS = 3\n",
    "    EPOCHS = 80\n",
    "    BATCH_SIZE = 64\n",
    "    STEPSIZE = 0.05\n",
    "    EARLY_STOPPING = 10\n",
    "\n",
    "PCA_COMPONENTS = N_QUBITS\n",
    "DEVICE_NAME = \"default.qubit\"  # statevector simulator\n",
    "DATA_FILES = [\"spam_ham_dataset.csv\", \"spam.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608b2703-59d7-4208-99b9-db8988e32f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data utilities\n",
    "# -----------------------------\n",
    "def load_and_merge(paths):\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            if VERBOSE:\n",
    "                print(f\"Info: {p} not found, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(p, encoding=\"utf-8\", low_memory=False)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(p, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "        if \"text\" in df.columns and (\"label\" in df.columns or \"label_num\" in df.columns):\n",
    "            if \"label_num\" not in df.columns:\n",
    "                df[\"label_num\"] = df[\"label\"].map(lambda s: 1 if str(s).strip().lower().startswith(\"spam\") else 0)\n",
    "            frames.append(df[[\"text\", \"label_num\"]].rename(columns={\"label_num\": \"label\"}))\n",
    "        elif \"v2\" in df.columns and \"v1\" in df.columns:\n",
    "            frames.append(pd.DataFrame({\n",
    "                \"text\": df[\"v2\"].astype(str),\n",
    "                \"label\": df[\"v1\"].apply(lambda s: 1 if str(s).strip().lower()==\"spam\" else 0)\n",
    "            }))\n",
    "        else:\n",
    "            cols = list(df.columns)\n",
    "            if len(cols) >= 2:\n",
    "                tmp = df[[cols[0], cols[1]]].rename(columns={cols[0]: \"text\", cols[1]: \"label\"})\n",
    "                try:\n",
    "                    tmp[\"label\"] = tmp[\"label\"].astype(int)\n",
    "                except Exception:\n",
    "                    tmp[\"label\"] = tmp[\"label\"].apply(lambda s: 1 if str(s).strip().lower().startswith(\"spam\") else 0)\n",
    "                frames.append(tmp)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(f\"No valid dataset found among {paths}\")\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    df_all[\"text\"] = df_all[\"text\"].astype(str).str.strip()\n",
    "    df_all = df_all.drop_duplicates(subset=[\"text\"])\n",
    "    df_all = df_all.dropna(subset=[\"text\", \"label\"])\n",
    "    df_all[\"label\"] = df_all[\"label\"].astype(int)\n",
    "    if VERBOSE:\n",
    "        print(\"Merged dataset shape:\", df_all.shape)\n",
    "    return df_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5627ce4-30c7-4cb9-b75c-fbb567a0ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    texts = df[\"text\"].values\n",
    "    labels = df[\"label\"].values\n",
    "    # standard split\n",
    "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(texts, labels, test_size=0.20, stratify=labels, random_state=RND)\n",
    "    X_train_txt, X_val_txt, y_train, y_val = train_test_split(X_train_txt, y_train, test_size=0.1765, stratify=y_train, random_state=RND)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfv = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words=\"english\")\n",
    "    X_train_tfidf = tfv.fit_transform(X_train_txt).toarray()\n",
    "    X_val_tfidf = tfv.transform(X_val_txt).toarray()\n",
    "    X_test_tfidf = tfv.transform(X_test_txt).toarray()\n",
    "\n",
    "    joblib.dump(tfv, os.path.join(ARTIFACTS, \"tfidf.joblib\"))\n",
    "    if VERBOSE:\n",
    "        print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape)\n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2e757f6-7dad-4691-b2c2-56d4cdf56b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Quantum model (compact)\n",
    "# -----------------------------\n",
    "dev = qml.device(DEVICE_NAME, wires=N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def qnode(q_weights, x):\n",
    "    # re-uploading: embed then apply one-layer StronglyEntanglingLayers repeatedly\n",
    "    for layer in range(q_weights.shape[0]):\n",
    "        qml.templates.AngleEmbedding(x, wires=range(N_QUBITS), rotation=\"X\")\n",
    "        # slice single layer\n",
    "        qml.templates.StronglyEntanglingLayers(q_weights[layer:layer+1], wires=range(N_QUBITS))\n",
    "    # return expectation of first qubit only (faster than all expvals)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a06400-fd9d-4558-b4aa-6d6acb570764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: using only one readout reduces runtime — often enough for binary tasks.\n",
    "# If you want higher expressivity, change to return all qubits' expvals.\n",
    "\n",
    "# -----------------------------\n",
    "# Flatten helpers and prediction\n",
    "# -----------------------------\n",
    "def pack_flat(q_weights, w_class, b):\n",
    "    return qnp.concatenate([q_weights.reshape(-1), qnp.array(w_class).reshape(-1), qnp.array([b])])\n",
    "\n",
    "def unpack_flat(flat):\n",
    "    q_cnt = N_LAYERS * N_QUBITS * 3\n",
    "    q_flat = flat[:q_cnt]\n",
    "    q_weights = q_flat.reshape((N_LAYERS, N_QUBITS, 3))\n",
    "    c_flat = flat[q_cnt:]\n",
    "    w_class = c_flat[:1]  # single weight because qnode returns one scalar\n",
    "    b = c_flat[1]\n",
    "    return q_weights, w_class, b\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + qnp.exp(-x))\n",
    "\n",
    "def batch_predict(flat_params, X):\n",
    "    q_weights, w_class, b = unpack_flat(flat_params)\n",
    "    outs = []\n",
    "    for x in X:\n",
    "        v = qnode(q_weights, x)  # scalar\n",
    "        z = w_class[0] * v + b\n",
    "        outs.append(sigmoid(z))\n",
    "    return qnp.array(outs, dtype=float)\n",
    "\n",
    "def batch_loss(flat_params, X, y):\n",
    "    preds = batch_predict(flat_params, X)\n",
    "    yq = qnp.array(y, dtype=float)\n",
    "    eps = 1e-7\n",
    "    return -qnp.mean(yq * qnp.log(preds + eps) + (1 - yq) * qnp.log(1 - preds + eps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e77c9c2-42ad-4d46-b655-cebb66340ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training loop (optimized)\n",
    "# -----------------------------\n",
    "def train_vqc(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # init smaller q_weights and 1 classical weight + bias\n",
    "    q_init = 0.01 * np.random.randn(N_LAYERS, N_QUBITS, 3)\n",
    "    w_init = np.array([0.1])  # single weight\n",
    "    b_init = 0.0\n",
    "\n",
    "    flat_params = pack_flat(qnp.array(q_init), w_init, b_init)\n",
    "\n",
    "    opt = qml.AdamOptimizer(stepsize=STEPSIZE)\n",
    "    best_val = -1.0\n",
    "    best_flat = flat_params\n",
    "    no_imp = 0\n",
    "    n = X_train.shape[0]\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"Start VQC training | epochs:\", EPOCHS, \"batch:\", BATCH_SIZE, \"stepsize:\", STEPSIZE)\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        idx = np.random.permutation(n)\n",
    "        Xs = X_train[idx]\n",
    "        ys = y_train[idx]\n",
    "        for i in range(0, n, BATCH_SIZE):\n",
    "            Xb = Xs[i:i+BATCH_SIZE]\n",
    "            yb = ys[i:i+BATCH_SIZE]\n",
    "            def cost(p): return batch_loss(p, Xb, yb)\n",
    "            flat_params = opt.step(cost, flat_params)\n",
    "\n",
    "        # validation\n",
    "        yv_prob = np.array(batch_predict(flat_params, X_val).tolist(), dtype=float)\n",
    "        yv_pred = (yv_prob >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_val, yv_pred)\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(f\"Epoch {ep+1}/{EPOCHS} - Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val + 1e-6:\n",
    "            best_val = val_acc\n",
    "            best_flat = flat_params\n",
    "            no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "\n",
    "        if no_imp >= EARLY_STOPPING:\n",
    "            if VERBOSE:\n",
    "                print(\"Early stopping at epoch\", ep+1)\n",
    "            break\n",
    "    # test eval\n",
    "    y_test_prob = np.array(batch_predict(best_flat, X_test).tolist(), dtype=float)\n",
    "    y_test_pred = (y_test_prob >= 0.5).astype(int)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    if VERBOSE:\n",
    "        print(\"VQC test acc:\", test_acc)\n",
    "        print(classification_report(y_test, y_test_pred))\n",
    "    joblib.dump({\"flat_params\": np.array(best_flat)}, os.path.join(ARTIFACTS, \"vqc_fast.joblib\"))\n",
    "    return test_acc, best_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18eba20f-6c81-4fc3-babc-498748648911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Classical baseline (fast)\n",
    "# -----------------------------\n",
    "def train_classical_svm(Xtr, ytr, Xval, yval, Xtest, ytest):\n",
    "    svc = SVC(kernel=\"linear\", C=1.0, probability=True, random_state=RND)\n",
    "    svc.fit(Xtr, ytr)\n",
    "    if VERBOSE:\n",
    "        print(\"Classical SVM val acc:\", accuracy_score(yval, svc.predict(Xval)))\n",
    "        print(\"Classical SVM test acc:\", accuracy_score(ytest, svc.predict(Xtest)))\n",
    "    joblib.dump(svc, os.path.join(ARTIFACTS, \"svm_fast.joblib\"))\n",
    "    return svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec0ae4fc-76a6-4e52-bffb-4e53b254c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (10151, 2)\n",
      "TF-IDF shapes: (6686, 500) (1434, 500) (2031, 500)\n",
      "Classical SVM val acc: 0.9504881450488145\n",
      "Classical SVM test acc: 0.9492860659773511\n",
      "Start VQC training | epochs: 20 batch: 128 stepsize: 0.12\n",
      "Epoch 1/20 - Val acc: 0.8501\n",
      "Epoch 2/20 - Val acc: 0.8668\n",
      "Epoch 3/20 - Val acc: 0.8591\n",
      "Epoch 4/20 - Val acc: 0.8668\n",
      "Epoch 5/20 - Val acc: 0.8640\n",
      "Epoch 6/20 - Val acc: 0.8480\n",
      "Early stopping at epoch 6\n",
      "VQC test acc: 0.8414574101427869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      1610\n",
      "           1       0.63      0.58      0.60       421\n",
      "\n",
      "    accuracy                           0.84      2031\n",
      "   macro avg       0.76      0.75      0.75      2031\n",
      "weighted avg       0.84      0.84      0.84      2031\n",
      "\n",
      "Total elapsed: 1264.4s\n",
      "VQC test acc: 0.8415, SVM test acc: 0.9493, Ensemble acc: 0.9424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main pipeline\n",
    "# -----------------------------\n",
    "def main():\n",
    "    t0 = time.time()\n",
    "    df = load_and_merge(DATA_FILES)\n",
    "    Xtr_tfidf, Xv_tfidf, Xt_tfidf, ytr, yv, yt = prepare_features(df)\n",
    "\n",
    "    # baseline\n",
    "    svc = train_classical_svm(Xtr_tfidf, ytr, Xv_tfidf, yv, Xt_tfidf, yt)\n",
    "\n",
    "    # PCA -> n_qubits\n",
    "    pca = PCA(n_components=PCA_COMPONENTS, random_state=RND)\n",
    "    Xtr_pca = pca.fit_transform(Xtr_tfidf)\n",
    "    Xv_pca = pca.transform(Xv_tfidf)\n",
    "    Xt_pca = pca.transform(Xt_tfidf)\n",
    "    joblib.dump(pca, os.path.join(ARTIFACTS, \"pca_fast.joblib\"))\n",
    "\n",
    "    # scale to [-pi, pi] for AngleEmbedding\n",
    "    scaler = MinMaxScaler(feature_range=(-np.pi, np.pi))\n",
    "    Xtr_pca = scaler.fit_transform(Xtr_pca)\n",
    "    Xv_pca = scaler.transform(Xv_pca)\n",
    "    Xt_pca = scaler.transform(Xt_pca)\n",
    "    joblib.dump(scaler, os.path.join(ARTIFACTS, \"scaler_fast.joblib\"))\n",
    "\n",
    "    # Convert arrays to dtype float for qnode\n",
    "    Xtr_pca = np.array(Xtr_pca, dtype=float)\n",
    "    Xv_pca = np.array(Xv_pca, dtype=float)\n",
    "    Xt_pca = np.array(Xt_pca, dtype=float)\n",
    "\n",
    "    # Train VQC\n",
    "    global N_QUBITS, N_LAYERS  # qnode references these\n",
    "    N_QUBITS = N_QUBITS  # already set\n",
    "    N_LAYERS = N_LAYERS  # already set\n",
    "\n",
    "    vqc_acc, best = train_vqc(Xtr_pca, ytr, Xv_pca, yv, Xt_pca, yt)\n",
    "    elapsed = time.time() - t0\n",
    "    if VERBOSE:\n",
    "        print(f\"Total elapsed: {elapsed:.1f}s\")\n",
    "\n",
    "    # Ensemble\n",
    "    vqc_probs = np.array(batch_predict(qnp.array(best), Xt_pca).tolist(), dtype=float)\n",
    "    svm_probs = svc.predict_proba(Xt_tfidf)[:, 1]\n",
    "    ensemble_probs = 0.5 * (vqc_probs + svm_probs)\n",
    "    ensemble_pred = (ensemble_probs >= 0.5).astype(int)\n",
    "    ensemble_acc = accuracy_score(yt, ensemble_pred)\n",
    "    if VERBOSE:\n",
    "        print(f\"VQC test acc: {vqc_acc:.4f}, SVM test acc: {accuracy_score(yt, svc.predict(Xt_tfidf)):.4f}, Ensemble acc: {ensemble_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae32d38-a4cf-40aa-aae7-069639489300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
